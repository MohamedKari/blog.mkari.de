<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  
  <title>Native Spark on Kubernetes</title>
  <meta property="og:title" content="Native Spark on Kubernetes" />
  <meta name="twitter:title" content="Native Spark on Kubernetes" />
  

  

  <meta name="author" content="Mo Kari"/>
  <meta property="og:site_name" content="" />
  <meta property="og:url" content="https://mohamedkari.github.io/blog.mkari.de/posts/spark-on-k8s/" />

  
  <meta name="twitter:card" content="summary" />

  

  
  <meta property="og:type" content="article" />
  
  
  
  <meta name="generator" content="Hugo 0.95.0" />
  
  

  <link rel="stylesheet" href="https://mohamedkari.github.io/blog.mkari.de/css/style.css" />
  
  
  
  <script type="text/javascript" src="https://mohamedkari.github.io/blog.mkari.de/js/bundle.js"></script>
  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          });
      });
  </script>
</head>

<body>
  <a href="#main" class="skip-link p-screen-reader-text">Skip to content</a>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;" aria-hidden="true"> <symbol id="icon-500px" viewBox="0 0 16 16"><g> <path d="M3.953 10.512a5.24 5.24 0 0 0 6.996 3.141c.625-.262 1.184-.641 1.666-1.122s.859-1.041 1.122-1.666c.272-.647.412-1.331.412-2.037s-.137-1.394-.412-2.037c-.262-.625-.641-1.184-1.122-1.666s-1.041-.859-1.666-1.122a5.226 5.226 0 0 0-2.037-.413c-.716 0-1.431.144-2.066.413-.509.216-1.372.769-1.875 1.291l-.003.003V.984h7.241c.262-.003.262-.372.262-.491 0-.122 0-.487-.266-.491H4.377a.343.343 0 0 0-.344.341v6.066c0 .197.244.338.472.384.444.094.544-.047.653-.197l.016-.019c.166-.247.681-.766.688-.772a4.262 4.262 0 0 1 3.037-1.25c1.147 0 2.222.444 3.028 1.25a4.245 4.245 0 0 1 1.256 3.019 4.236 4.236 0 0 1-1.25 3.019 4.336 4.336 0 0 1-3.047 1.25 4.136 4.136 0 0 1-2.159-.597l.003-3.688c0-.491.213-1.028.572-1.431a2.09 2.09 0 0 1 1.588-.716c.594 0 1.15.225 1.566.634.409.406.637.95.637 1.528a2.179 2.179 0 0 1-2.206 2.197c-.238 0-.672-.106-.691-.109-.25-.075-.356.272-.391.387-.134.441.069.528.109.541.397.125.659.147 1.003.147a3.173 3.173 0 0 0 3.169-3.169c0-1.734-1.422-3.144-3.166-3.144-.856 0-1.659.328-2.263.919-.575.566-.903 1.319-.903 2.069v.019c-.003.094-.003 2.306-.006 3.031l-.003-.003c-.328-.363-.653-.919-.869-1.488-.084-.222-.275-.184-.534-.103-.125.034-.469.141-.391.394zm3.722-.865c0 .106.097.2.156.253l.019.019c.1.097.194.147.281.147a.181.181 0 0 0 .131-.05c.044-.041.537-.544.588-.591l.553.55c.05.056.106.088.172.088.088 0 .184-.053.284-.156.238-.244.119-.375.063-.438l-.559-.559.584-.588c.128-.137.016-.284-.097-.397-.162-.162-.322-.206-.422-.112l-.581.581-.588-.588a.16.16 0 0 0-.113-.047c-.078 0-.172.053-.275.156-.181.181-.219.306-.125.406l.588.584-.584.584c-.053.05-.078.103-.075.156zm1.278-7.931c-.938 0-1.938.191-2.669.506a.207.207 0 0 0-.134.181.753.753 0 0 0 .069.337c.047.116.166.425.4.334a6.689 6.689 0 0 1 2.334-.444 6.35 6.35 0 0 1 2.469.497c.622.263 1.206.644 1.844 1.194a.22.22 0 0 0 .147.059c.125 0 .244-.122.347-.237.169-.191.287-.35.119-.509a6.858 6.858 0 0 0-2.1-1.356 7.326 7.326 0 0 0-2.825-.563zM14.006 13.3c-.113-.113-.209-.178-.294-.203s-.162-.006-.222.053l-.056.056a6.32 6.32 0 0 1-6.938 1.356 6.336 6.336 0 0 1-2.013-1.356 6.046 6.046 0 0 1-1.356-2.012c-.288-.713-.381-1.247-.413-1.422-.003-.016-.006-.028-.006-.037-.041-.206-.231-.222-.503-.178-.112.019-.459.072-.428.319v.006a7.261 7.261 0 0 0 2.04 3.994 7.266 7.266 0 0 0 10.288 0l.059-.059c.069-.084.134-.225-.159-.516z"/> </g></symbol> <symbol id="icon-codepen" viewBox="0 0 16 16"><g> <path d="M14.777 5.751l-7-4.667a.5.5 0 0 0-.555 0l-7 4.667a.501.501 0 0 0-.223.416v4.667c0 .167.084.323.223.416l7 4.667a.5.5 0 0 0 .554 0l7-4.667a.501.501 0 0 0 .223-.416V6.167a.501.501 0 0 0-.223-.416zM7.5 10.232L4.901 8.5 7.5 6.768 10.099 8.5 7.5 10.232zM8 5.899V2.434l5.599 3.732L11 7.898l-3-2zm-1 0l-3 2-2.599-1.732L7 2.435V5.9zM3.099 8.5L1 9.899V7.101L3.099 8.5zM4 9.101l3 2v3.465l-5.599-3.732L4 9.102zm4 2l3-2 2.599 1.732L8 14.565V11.1zM11.901 8.5L14 7.101v2.798L11.901 8.5z"/> </g></symbol> <symbol id="icon-dribbble" viewBox="0 0 16 16"><g> <path d="M8 16c-4.412 0-8-3.588-8-8s3.587-8 8-8c4.412 0 8 3.587 8 8s-3.588 8-8 8zm6.747-6.906c-.234-.075-2.116-.634-4.256-.291a29.7 29.7 0 0 1 1.328 4.872 6.845 6.845 0 0 0 2.928-4.581zM10.669 14.3c-.103-.6-.497-2.688-1.456-5.181-.016.006-.031.009-.044.016-3.856 1.344-5.241 4.016-5.362 4.266a6.807 6.807 0 0 0 6.863.9zm-7.747-1.722c.156-.266 2.031-3.369 5.553-4.509a7.04 7.04 0 0 1 .269-.081 24.04 24.04 0 0 0-.553-1.159c-3.409 1.022-6.722.978-7.022.975-.003.069-.003.138-.003.209 0 1.753.666 3.356 1.756 4.566zM1.313 6.609c.306.003 3.122.016 6.319-.831a43.092 43.092 0 0 0-2.534-3.953 6.854 6.854 0 0 0-3.784 4.784zM6.4 1.366a36.612 36.612 0 0 1 2.55 4c2.431-.909 3.459-2.294 3.581-2.469A6.799 6.799 0 0 0 6.4 1.366zm6.891 2.325c-.144.194-1.291 1.663-3.816 2.694.159.325.313.656.453.991.05.119.1.234.147.353 2.275-.284 4.534.172 4.759.219a6.816 6.816 0 0 0-1.544-4.256z"/> </g></symbol> <symbol id="icon-facebook" viewBox="0 0 16 16"><g> <path d="M9.5 3H12V0H9.5C7.57 0 6 1.57 6 3.5V5H4v3h2v8h3V8h2.5l.5-3H9V3.5c0-.271.229-.5.5-.5z"/> </g></symbol> <symbol id="icon-feed" viewBox="0 0 16 16"><g> <path d="M2.13 11.733c-1.175 0-2.13.958-2.13 2.126 0 1.174.955 2.122 2.13 2.122a2.126 2.126 0 0 0 2.133-2.122 2.133 2.133 0 0 0-2.133-2.126zM.002 5.436v3.067c1.997 0 3.874.781 5.288 2.196a7.45 7.45 0 0 1 2.192 5.302h3.08c0-5.825-4.739-10.564-10.56-10.564zM.006 0v3.068C7.128 3.068 12.924 8.87 12.924 16H16C16 7.18 8.824 0 .006 0z"/> </g></symbol> <symbol id="icon-flickr" viewBox="0 0 16 16"><g> <path d="M0 8.5a3.5 3.5 0 1 1 7 0 3.5 3.5 0 0 1-7 0zm9 0a3.5 3.5 0 1 1 7 0 3.5 3.5 0 0 1-7 0z"/> </g></symbol> <symbol id="icon-github" viewBox="0 0 16 16"><g> <path d="M8 .198a8 8 0 0 0-2.529 15.591c.4.074.547-.174.547-.385 0-.191-.008-.821-.011-1.489-2.226.484-2.695-.944-2.695-.944-.364-.925-.888-1.171-.888-1.171-.726-.497.055-.486.055-.486.803.056 1.226.824 1.226.824.714 1.223 1.872.869 2.328.665.072-.517.279-.87.508-1.07-1.777-.202-3.645-.888-3.645-3.954 0-.873.313-1.587.824-2.147-.083-.202-.357-1.015.077-2.117 0 0 .672-.215 2.201.82A7.672 7.672 0 0 1 8 4.066c.68.003 1.365.092 2.004.269 1.527-1.035 2.198-.82 2.198-.82.435 1.102.162 1.916.079 2.117.513.56.823 1.274.823 2.147 0 3.073-1.872 3.749-3.653 3.947.287.248.543.735.543 1.481 0 1.07-.009 1.932-.009 2.195 0 .213.144.462.55.384A8 8 0 0 0 8.001.196z"/> </g></symbol> <symbol id="icon-gitlab" viewBox="0 0 28 28"><g> <path d="M1.625 11.031L14 26.89.437 17.046a1.092 1.092 0 0 1-.391-1.203l1.578-4.813zm7.219 0h10.313L14.001 26.89zM5.75 1.469l3.094 9.562H1.625l3.094-9.562a.548.548 0 0 1 1.031 0zm20.625 9.562l1.578 4.813a1.09 1.09 0 0 1-.391 1.203l-13.563 9.844 12.375-15.859zm0 0h-7.219l3.094-9.562a.548.548 0 0 1 1.031 0z"/> </g></symbol> <symbol id="icon-google-plus" viewBox="0 0 16 16"><g> <path d="M5.091 7.147v1.747h2.888c-.116.75-.872 2.197-2.888 2.197-1.737 0-3.156-1.441-3.156-3.216s1.419-3.216 3.156-3.216c.991 0 1.65.422 2.028.784L8.5 4.112c-.888-.828-2.037-1.331-3.409-1.331C2.275 2.784 0 5.059 0 7.875s2.275 5.091 5.091 5.091c2.937 0 4.888-2.066 4.888-4.975 0-.334-.037-.591-.081-.844H5.092zM16 7h-1.5V5.5H13V7h-1.5v1.5H13V10h1.5V8.5H16z"/> </g></symbol> <symbol id="icon-instagram" viewBox="0 0 22 22"><g> <path d="M15.445 0H6.554A6.559 6.559 0 0 0 0 6.554v8.891A6.559 6.559 0 0 0 6.554 22h8.891a6.56 6.56 0 0 0 6.554-6.555V6.554A6.557 6.557 0 0 0 15.445 0zm4.342 15.445a4.343 4.343 0 0 1-4.342 4.342H6.554a4.341 4.341 0 0 1-4.341-4.342V6.554a4.34 4.34 0 0 1 4.341-4.341h8.891a4.342 4.342 0 0 1 4.341 4.341l.001 8.891z"/> <path d="M11 5.312A5.693 5.693 0 0 0 5.312 11 5.694 5.694 0 0 0 11 16.688 5.694 5.694 0 0 0 16.688 11 5.693 5.693 0 0 0 11 5.312zm0 9.163a3.475 3.475 0 1 1-.001-6.95 3.475 3.475 0 0 1 .001 6.95zm5.7-10.484a1.363 1.363 0 1 1-1.364 1.364c0-.752.51-1.364 1.364-1.364z"/> </g></symbol> <symbol id="icon-linkedin" viewBox="0 0 16 16"><g> <path d="M6 6h2.767v1.418h.04C9.192 6.727 10.134 6 11.539 6 14.46 6 15 7.818 15 10.183V15h-2.885v-4.27c0-1.018-.021-2.329-1.5-2.329-1.502 0-1.732 1.109-1.732 2.255V15H6V6zM1 6h3v9H1V6zM4 3.5a1.5 1.5 0 1 1-3.001-.001A1.5 1.5 0 0 1 4 3.5z"/> </g></symbol> <symbol id="icon-mail" viewBox="0 0 22 18"><g> <path fill="#000" d="M0 17.225V.776h22v16.447H0v.002zm3.011-1.815h15.978l-5.111-5.115L11 13.179l-2.877-2.883-5.112 5.114zm-1.216-1.275l5.077-5.09L1.795 3.98v10.155zm13.332-5.09l5.079 5.09V3.979l-5.079 5.066zm-4.126 1.588l8.022-8.027-16.045-.001 8.023 8.028z"/> </g></symbol> <symbol id="icon-medium" viewBox="0 0 24 24"><g> <path d="M22.085 4.733L24 2.901V2.5h-6.634l-4.728 11.768L7.259 2.5H.303v.401L2.54 5.594c.218.199.332.49.303.783V16.96c.069.381-.055.773-.323 1.05L0 21.064v.396h7.145v-.401l-2.52-3.049a1.244 1.244 0 0 1-.347-1.05V7.806l6.272 13.659h.729l5.393-13.659v10.881c0 .287 0 .346-.188.534l-1.94 1.877v.402h9.412v-.401l-1.87-1.831a.556.556 0 0 1-.214-.534V5.267a.554.554 0 0 1 .213-.534z"/> </g></symbol> <symbol id="icon-npm" viewBox="0 0 16 16"><g> <path d="M0 0v16h16V0H0zm13 13h-2V5H8v8H3V3h10v10z"/> </g></symbol> <symbol id="icon-pinterest" viewBox="0 0 16 16"><g> <path d="M8 1.069a6.93 6.93 0 0 0-2.525 13.384c-.059-.547-.116-1.391.025-1.988.125-.541.813-3.444.813-3.444s-.206-.416-.206-1.028c0-.963.559-1.684 1.253-1.684.591 0 .878.444.878.975 0 .594-.378 1.484-.575 2.306-.166.691.344 1.253 1.025 1.253 1.231 0 2.178-1.3 2.178-3.175 0-1.659-1.194-2.819-2.894-2.819-1.972 0-3.128 1.478-3.128 3.009 0 .597.228 1.234.516 1.581.056.069.066.128.047.2a95.89 95.89 0 0 1-.194.787c-.031.128-.1.153-.231.094-.866-.403-1.406-1.669-1.406-2.684 0-2.188 1.587-4.194 4.578-4.194 2.403 0 4.272 1.712 4.272 4.003 0 2.388-1.506 4.313-3.597 4.313-.703 0-1.362-.366-1.588-.797 0 0-.347 1.322-.431 1.647-.156.603-.578 1.356-.862 1.816a6.93 6.93 0 0 0 8.984-6.622 6.931 6.931 0 0 0-6.931-6.934z"/> </g></symbol> <symbol id="icon-search" viewBox="0 0 16 16"><g> <path d="M15.504 13.616l-3.79-3.223c-.392-.353-.811-.514-1.149-.499a6 6 0 1 0-.672.672c-.016.338.146.757.499 1.149l3.223 3.79c.552.613 1.453.665 2.003.115s.498-1.452-.115-2.003zM6 10a4 4 0 1 1 0-8 4 4 0 0 1 0 8z"/> </g></symbol> <symbol id="icon-tumblr" viewBox="0 0 16 16"><g> <path d="M9.001 7v3.659c0 .928-.012 1.463.086 1.727.098.262.342.534.609.691.354.212.758.318 1.214.318.81 0 1.289-.107 2.09-.633v2.405a9.089 9.089 0 0 1-1.833.639A7.93 7.93 0 0 1 9.369 16a4.9 4.9 0 0 1-1.725-.276 4.195 4.195 0 0 1-1.438-.79c-.398-.343-.672-.706-.826-1.091s-.23-.944-.23-1.676V6.556H3.003V4.29c.628-.204 1.331-.497 1.778-.877a4.386 4.386 0 0 0 1.08-1.374C6.133 1.505 6.32.825 6.422 0h2.579v4H13v3H9.001z"/> </g></symbol> <symbol id="icon-twitter" viewBox="0 0 16 16"><g> <path d="M16 3.538a6.461 6.461 0 0 1-1.884.516 3.301 3.301 0 0 0 1.444-1.816 6.607 6.607 0 0 1-2.084.797 3.28 3.28 0 0 0-2.397-1.034 3.28 3.28 0 0 0-3.197 4.028 9.321 9.321 0 0 1-6.766-3.431 3.284 3.284 0 0 0 1.015 4.381A3.301 3.301 0 0 1 .643 6.57v.041A3.283 3.283 0 0 0 3.277 9.83a3.291 3.291 0 0 1-1.485.057 3.293 3.293 0 0 0 3.066 2.281 6.586 6.586 0 0 1-4.862 1.359 9.286 9.286 0 0 0 5.034 1.475c6.037 0 9.341-5.003 9.341-9.341 0-.144-.003-.284-.009-.425a6.59 6.59 0 0 0 1.637-1.697z"/> </g></symbol> <symbol id="icon-vimeo" viewBox="0 0 16 16"><g> <path d="M15.994 4.281c-.072 1.556-1.159 3.691-3.263 6.397-2.175 2.825-4.016 4.241-5.522 4.241-.931 0-1.722-.859-2.366-2.581-.431-1.578-.859-3.156-1.291-4.734-.478-1.722-.991-2.581-1.541-2.581-.119 0-.538.253-1.256.753l-.753-.969c.791-.694 1.569-1.388 2.334-2.081 1.053-.909 1.844-1.387 2.372-1.438 1.244-.119 2.013.731 2.3 2.553.309 1.966.525 3.188.647 3.666.359 1.631.753 2.447 1.184 2.447.334 0 .838-.528 1.509-1.588.669-1.056 1.028-1.862 1.078-2.416.097-.912-.262-1.372-1.078-1.372a2.98 2.98 0 0 0-1.184.263c.787-2.575 2.287-3.825 4.506-3.753 1.641.044 2.416 1.109 2.322 3.194z"/> </g></symbol> <symbol id="icon-wordpress" viewBox="0 0 16 16"><g> <path d="M2 8c0 2.313 1.38 4.312 3.382 5.259L2.52 5.622A5.693 5.693 0 0 0 2 8zm10.05-.295c0-.722-.266-1.222-.495-1.612-.304-.482-.589-.889-.589-1.371 0-.537.418-1.037 1.008-1.037.027 0 .052.003.078.005A6.064 6.064 0 0 0 8 2.156 6.036 6.036 0 0 0 2.987 4.79c.141.004.274.007.386.007.627 0 1.599-.074 1.599-.074.323-.018.361.444.038.482 0 0-.325.037-.687.055l2.185 6.33 1.313-3.835-.935-2.495a12.304 12.304 0 0 1-.629-.055c-.323-.019-.285-.5.038-.482 0 0 .991.074 1.58.074.627 0 1.599-.074 1.599-.074.323-.018.362.444.038.482 0 0-.326.037-.687.055l2.168 6.282.599-1.947c.259-.809.457-1.389.457-1.889zm-3.945.806l-1.8 5.095a6.148 6.148 0 0 0 3.687-.093.52.52 0 0 1-.043-.081L8.105 8.511zm5.16-3.315c.026.186.04.386.04.601 0 .593-.114 1.259-.456 2.093l-1.833 5.16c1.784-1.013 2.983-2.895 2.983-5.051a5.697 5.697 0 0 0-.735-2.803zM8 0a8 8 0 1 0 0 16A8 8 0 0 0 8 0zm0 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14z"/> </g></symbol> <symbol id="icon-youtube" viewBox="0 0 16 16"><g> <path d="M15.841 4.8s-.156-1.103-.637-1.587c-.609-.637-1.291-.641-1.603-.678-2.237-.163-5.597-.163-5.597-.163h-.006s-3.359 0-5.597.163c-.313.038-.994.041-1.603.678C.317 3.697.164 4.8.164 4.8S.005 6.094.005 7.391v1.213c0 1.294.159 2.591.159 2.591s.156 1.103.634 1.588c.609.637 1.409.616 1.766.684 1.281.122 5.441.159 5.441.159s3.363-.006 5.6-.166c.313-.037.994-.041 1.603-.678.481-.484.637-1.588.637-1.588s.159-1.294.159-2.591V7.39c-.003-1.294-.162-2.591-.162-2.591zm-9.494 5.275V5.578l4.322 2.256-4.322 2.241z"/> </g></symbol></svg>
  <header class="l-header" style="max-width: 720px; margin: auto; text-align: left; padding: 20px 0 0 0 ;">
    
    <p class="home-button"><a href="https://mohamedkari.github.io/blog.mkari.de/" class="p-title__link">‚Üê Back to home</a></p>
    <p class="c-title p-title"><a href="https://mohamedkari.github.io/blog.mkari.de/" class="p-title__link"></a></p>
    
    
  </header>

  <main id="main" class="l-main">


<article class="p-article">
  <header>
    <h1>Native Spark on Kubernetes</h1>
    <div>
      <div class="c-time">
        Posted on
<time datetime="2020-10-06T00:00:00Z">
  Oct 6, 2020
</time>

      </div>
      
      <div class="c-time">
        by Mo Kari
      </div>
    </div>
  </header>
  
  <section id="js-article" class="p-article__body">
    <p><em>In this post, the different deployment alternatives of Spark on Kubernetes are evaluated.</em>
<em>From this, I&rsquo;ll outline the workflow for building and running Spark Applications as well as Spark Cluster-backed Jupyter Notebooks, both running PySpark in custom containers.</em>
<em>It is shown how to include conda-managed Python dependencies in the image.</em>
<em>Also, it is described how to deploy a notebook server running in Spark&rsquo;s client mode to the Kubernetes cluster.</em>
<em>Workloads use AWS S3 as the data source and sink and are observable using the Spark history server.</em></p>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#development-environment">Development Environment</a></li>
<li><a href="#spark-terminology">Spark Terminology</a></li>
<li><a href="#spark-on-k8s-deployment-alternatives">Spark on K8s Deployment Alternatives</a>
<ul>
<li><a href="#native-spark-on-k8s">Native Spark on K8s</a></li>
<li><a href="#spark-operator-for-spark-on-k8s">Spark Operator for Spark on K8s</a></li>
<li><a href="#spark-standalone-on-kubernetes">Spark Standalone on Kubernetes</a></li>
</ul>
</li>
<li><a href="#the-spark-container-image">The Spark Container Image</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#custom-spark-base-image-build">Custom Spark Base Image Build</a></li>
<li><a href="#custom-spark-application-image">Custom Spark Application Image</a></li>
</ul>
</li>
<li><a href="#running-custom-spark-applications-and-notebooks-on-k8s">Running Custom Spark Applications and Notebooks on K8s</a>
<ul>
<li><a href="#prerequisites">Prerequisites</a>
<ul>
<li><a href="#role">Role</a></li>
<li><a href="#log-files">Log files</a></li>
<li><a href="#secrets">Secrets</a></li>
</ul>
</li>
<li><a href="#submitting-applications-in-cluster-mode">Submitting Applications in Cluster Mode</a></li>
<li><a href="#running-notebooks-in-client-mode">Running Notebooks in Client Mode</a>
<ul>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#repl-shell">REPL Shell</a></li>
<li><a href="#jupyter-notebook">Jupyter Notebook</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-driver-ui-and-the-history-server">Spark Driver UI and the History Server</a></li>
<li><a href="#outlook">Outlook</a></li>
</ul>
<h1 id="development-environment">Development Environment</h1>
<p>I set up a template repo for Spark on Kubernetes under <a href="https://github.com/MohamedKari/spark-on-k8s">https://github.com/MohamedKari/spark-on-k8s</a>, corresponding to the workflow described here. I suggest taking a look at the repo before reading the text so that you can build a mental model of what we&rsquo;re working towards.</p>
<p>My environment is the following:</p>
<ul>
<li>OS X 10.15.4</li>
<li>Docker 19.03.13 (e. g. <code>brew cask install docker</code>)</li>
<li>Minikube v1.13.1 (<code>brew install minikube</code>)</li>
<li>Apache Spark 3.0.1 with PySpark (<code>brew install apache-spark</code>)</li>
<li>Helm v3.3.4 (<code>brew install helm</code>)</li>
<li>Minikube is serving Kubernetes v1.19.2 (<code>minikube --memory 8192 --cpus 4 start &amp;&amp; kubectl version</code>)</li>
<li>conda 4.8.3 (<code>brew cask install anaconda</code>)</li>
<li>awscli 2.0.34 (<code>brew install awscli</code>)</li>
</ul>
<p>Further, I have created image repos on Docker hub at <a href="https://hub.docker.com/repository/docker/mokari94/spark-base">mokari94/spark-base</a> and <a href="https://hub.docker.com/repository/docker/mokari94/spark-app">mokari94/spark-app</a> and am logged in to the registry (<code>docker login</code>). My AWS IAM role is allowed all actions on an S3 bucket and all objects inside:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>, 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;Action&#34;</span>: <span style="color:#e6db74">&#34;s3:*&#34;</span>, 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;Resource&#34;</span>: [ 
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;arn:aws:s3:::spark-mo&#34;</span>, 
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;arn:aws:s3:::spark-mo/*&#34;</span>
</span></span><span style="display:flex;"><span>  ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>()</p>
<h1 id="spark-terminology">Spark Terminology</h1>
<p>A central notion of a Spark application, may it be an interactive notebook, an end-to-end application, is the <em>driver process</em>.
The driver process could run on the developer&rsquo;s machine when using a spark-shell or a locally started notebook, it could run on a gateway host at the edge of a cluster, on a node in the cluster itself, or on a notebook server in the cluster.
We&rsquo;ll come back to where the driver process runs, in a bit.</p>
<p>The <em>driver process</em> distributes a workload across a set of <em>executors</em>.
These executors are processes, running on nodes in the cluster.
How does the driver process allocate executors on the nodes of the cluster?
The answer is the <em>cluster manager</em>.
Upon startup, the submitting process will instruct the cluster manager to create a certain number of executors on the cluster.</p>
<p>In the so-called <em>client mode</em>, the driver is identical to the submitting process, e. g. the developer machine or a gateway machine.
In the so-called <em>cluster mode</em>, the driver program gets deployed to the cluster by the cluster manager first, and then is started co-located to the executors, thus improving latency and being independent of the submitting machine.</p>
<p>The cluster manager will then start each executor and tell them the IP and port of the driver process.
Of course, the cluster manager needs to know the hosts of the cluster and must be able to start processes on them, generally by using an agent running on the node (even tough, there it might also be possible that the cluster manager can scale out the cluster itself by means of the cloud provider, as for example with <a href="https://github.com/kubernetes/autoscaler">Kubernetes Cluster Autoscaler</a>).
Once all executors have reported back to the driver, the driver can distribute <em>tasks</em> among them, e. g. tasks for data transformation.
Instead of sending the data through the driver over the network to the executors, the driver tells the executors where to find the data on a shared file system.
This could be an object store such as S3, a distributed file system such as HDFS, or a mounted file system.
Spark relies on data parallelism, that is each executor handles a partition of the overall input data.
The driver process is in charge of splitting the overall input data into more or less equally-sized partitions that then get assigned to the executors.
Instead of dumping the transformation result immediately back to the shared file system, the executors keep the result of a transformation in-memory until the driver either instructs the executors to dump it, or to aggregate the data.
Aggregation could mean that each executor reduces the partition it &ldquo;owns&rdquo; to a local result and returns this local result to the driver which then reduces all received results to a global result. Or it could mean that executors need to exchange data between each other before, in the so-called shuffle procedure.</p>
<p>In <em>Spark Standalone</em>, a cluster is managed by the so-called Spark Master process. By default, it is listening on port 7077.
Each so-called worker node of the cluster needs to run the Spark Worker Daemon process which - upon setup - is registered with the Spark Master running on the master node on port 7077.
Spark applications are then submitted to the cluster through the Spark Master using <code>spark-submit</code> script with the <code>--master spark://$SPARK_MASTER_HOST:7077</code> argument. Note that, even though in the following we will use the <code>--master</code> argument for non-Spark Standalone deployments, the terminology of <em>the</em> Spark Master process and <em>the</em> Spark Worker Nodes is specific to Spark Standalone.</p>
<h1 id="spark-on-k8s-deployment-alternatives">Spark on K8s Deployment Alternatives</h1>
<h2 id="native-spark-on-k8s">Native Spark on K8s</h2>
<p>Since 2018, Spark has <a href="https://issues.apache.org/jira/browse/SPARK-18278">native support for Kubernetes</a>.
For <em>Spark on K8s</em>, we assume that there is a Kubernetes cluster, serving the Kubernetes API at a given URL.
Spark applications are again submitted to the cluster using the <code>spark-submit</code> script, however this time indicating the <code>--master k8s://$K8S_API_URL</code> argument. Furthermore, one indicates <code>--conf spark.kubernetes.container.image</code> pointing to a pullable container image.
Now, instead of asking a running Spark Master to launch executor processes on pre-registered machines,
the local submission client, running in the local process invoked by <code>spark-submit</code>, will create a Kubernetes spec with as many pods as desired executors. The spec is sent to the  Kubernetes API and handled by the Kubernetes scheduler. The scheduler schedules the pods, each of them running a single container which runs the Spark executor.
In Spark 3.0.1, Spark Standalone as well as Spark on K8s can run handle cluster mode and client mode (with the exception of PySpark jobs running in cluster mode on Spark Standalone clusters).
This setup allows to run <em>custom</em> Spark containers on Kubernetes without any prior setup, apart from some role stuff.</p>
<h2 id="spark-operator-for-spark-on-k8s">Spark Operator for Spark on K8s</h2>
<p>However, running <code>spark-submit</code> in an imperative manner is a bit inconsistent with the declarative <code>kubectl apply</code> paradigm.
Therefore, the <a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator">Kubernetes Operator for Spark</a> has been developed.
As stated in the <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Kubernetes User Guide</a>, Kubernetes &ldquo;Operators are clients of the Kubernetes API that act as controllers for a Custom Resource&rdquo;.
The operator introduces a <code>SparkApplication</code> object kind that can be specified in a yaml file and applied using kubectl.
The corresponding <a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#architecture">controller creates the SparkApplication resource</a> by calling <code>spark-submit</code>.
So under the hood, the operator uses the above described native Spark on K8s, but wraps it in the Kubernetes-integrated API.
It can easily be installed through Helm.
By design, the Spark Operator doesn&rsquo;t support client mode, because the one who calls spark-submit is the operator itself.
Client-mode-like applications, e. g. the REPL, can still simply bypass the operator using <code>--master k8s://https://...</code>.</p>
<h2 id="spark-standalone-on-kubernetes">Spark Standalone on Kubernetes</h2>
<p>A bit oxymoronically, one can also deploy Spark as <em>Spark Standalone on Kubernetes</em>, e. g. using the <a href="https://github.com/bitnami/charts/tree/master/bitnami/spark">bitnami/spark</a> or <a href="https://github.com/helm/charts/tree/master/stable/spark">stable/spark</a> Helm charts. This has been the way to go for deploying on Kubernetes before there was native support. Instead of installing Spark Standalone on a group of e. g. EC2 instances, one would virtualize the machines as containers running on Kubernetes. In so doing, one would spark-submit with <code>--master spark://</code> (instead of <code>--master k8s://</code>) to a Spark Standalone Master which would then spawn executors on containerized worker nodes. Because Spark Standalone simply works with the nodes that have been registered, it is not possible to directly request new resources for a given job. Instead, scaling <a href="https://github.com/bitnami/charts/blob/master/bitnami/spark/templates/hpa-worker.yaml">is generally based on CPU and memory utilization</a>. In contrast to native K8s support, where the container is created on-demand using the container image given as a parameter to <code>spark-submit</code>, thus allowing to adapt executor context to the specific application, in Spark Standalone on Kubernetes, all applications share the same pre-deployed workers. The figure below summarizes the different deployment styles.</p>
<p><img src="fig-spark-on-k8s-comparison.png" alt=""></p>
<h1 id="the-spark-container-image">The Spark Container Image</h1>
<h2 id="overview">Overview</h2>
<p>One central advantage of using Spark on Kubernetes over Spark Standalone is that each application can bring its container comprising all dependencies which then can integrate with the typical Kubernetes features (e. g. ConfigMaps for managing configs).</p>
<p>There are two alternatives when it comes the container image: Either building the image oneself, or using an already published image.</p>
<p>As said above, the images corresponding to the Helm charts of bitnami/spark (<a href="https://hub.docker.com/r/bitnami/spark/">https://hub.docker.com/r/bitnami/spark/</a>, <a href="https://github.com/bitnami/bitnami-docker-spark/blob/master/3/debian-10/Dockerfile">Dockerfile</a>) and stable/spark image (k8s.gcr.io/spark:1.5.1_v3) are meant for Spark Standalone on K8s deployments.
However, the gcr.io/spark-operator/spark-py:v3.0.0 from the spark-operator image repo (<a href="https://console.cloud.google.com/gcr/images/spark-operator/GLOBAL">Registry Repo</a>, Dockerfile built using the official Spark <a href="https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile">Dockerfile for Kubernetes</a>) is meant for K8s-native deployments. Even though it bears the name spark-operator in its tag, it is a bog-standard Spark image.
Unfortunately, the dependencies it carries are not the newest (it features the Hadoop libraries in version 2.7.4) and it also doesn&rsquo;t come with libraries, we most certainly will need (e. g. in order to interact with AWS S3).</p>
<p>The main problem of older versions in the container, is that your local brew-installed Spark installation may contain libraries in a more recent version (as is the case for me).
This means, that there most certainly will arise runtime errors when deploying in client mode from you developer machine (because the driver and the executors then run different Spark versions).</p>
<p>Therefore, there are three alternatives:</p>
<ol>
<li>Don&rsquo;t use client mode (this also means: don&rsquo;t use the REPL or a local Jupyter)</li>
<li>&ldquo;Turn the knob on the driver side&rdquo; by ensuring you have the same versions as the executors.</li>
<li>&ldquo;Turn the knob on the executor side&rdquo; by building a custom image</li>
</ol>
<p>Alternative 1 is highly dissatisfying and makes developing slow.
Alternative 2 could probably work by running the spark-operator image in Docker.</p>
<p>However, let&rsquo;s go with Alternative 3 so we don&rsquo;t make ourselves guilty of running obsolete dependencies. In the following, we will build the image off of the local Spark installation which then gets deployed. Of course, for a stable CI/CDable production, this could also happen in a build runner.</p>
<h2 id="custom-spark-base-image-build">Custom Spark Base Image Build</h2>
<p>So, let&rsquo;s build the PySpark image from scratch. Actually, not really from scratch, because we use the <a href="(https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile)">Dockerfile for Kubernetes</a> maintained by the Spark community, and because we use the <a href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh">Docker image tool</a> that comes along to create the build context from a local Spark distribution.</p>
<p>This could be a dedicated Spark distribution, built from source using the
<a href="https://github.com/apache/spark/blob/master/build/mvn">build</a> and <a href="https://github.com/apache/spark/blob/master/dev/make-distribution.sh">distribution</a> scripts. This is the way to go when using a dedicated build runner or maybe a Docker multi-stage build. Then, we could also maintain our dependencies in the <a href="https://github.com/apache/spark/blob/master/pom.xml">dependency configuration</a>.</p>
<p>The standard Spark distribution, however, comes without S3 support, which depends on the <code>hadoop-aws</code> package. Normally, we would have to add this dependency into the <code>pom.xml</code>, but because we&rsquo;re not building from source, we have to add it manually. It can be found in the Maven repo. The library needs to be in the same version as all other Hadoop libraries in the Spark distribution.</p>
<p>Running</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>export SPARK_HOME<span style="color:#f92672">=</span>/usr/local/Cellar/apache-spark/3.0.1/libexec
</span></span><span style="display:flex;"><span>ls $SPARK_HOME/jars/<span style="color:#e6db74">&#34; |grep hadoop
</span></span></span></code></pre></div><p>reveals that the Hadoop libs in version 3.2.0 are used. The package page (<a href="https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.0">https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.0</a>) shows that it this depends on the <code>AWS Java SDK Bundle</code> in version 1.11.375. Older Hadoop AWS libs instead depend on the <code>AWS Java SDK</code>. So, one has to be careful here when adding these to <code>$SPARK_HOME/jars</code>.</p>
<p>We build the Spark Base Dockerfile by:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>export REPO<span style="color:#f92672">=</span>mokari94
</span></span><span style="display:flex;"><span>export TAG<span style="color:#f92672">=</span>latest
</span></span><span style="display:flex;"><span>export SPARK_HOME<span style="color:#f92672">=</span>/usr/local/Cellar/apache-spark/3.0.1/libexec
</span></span><span style="display:flex;"><span>cd $SPARK_HOME
</span></span><span style="display:flex;"><span>bin/docker-image-tool.sh -r $REPO/spark-base -t $TAG -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
</span></span></code></pre></div><p>This will build two pure spark image without any application-specific code, tagged <code>$REPO/spark-base/spark:latest</code> and <code>$REPO/spark-base/spark-py:latest</code>. Because we&rsquo;re interested in PySpark, let&rsquo;s use only the latter. Unfortunately, while it works with AWS ECR, pushing the image to Docker Hub fails with this tag, maybe due to the slash in the image name, since retagging works:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker tag $REPO/spark-base/spark-py:latest $REPO/spark-base:latest
</span></span><span style="display:flex;"><span>docker push $REPO/spark-base:latest
</span></span></code></pre></div><p>This is already a functioning Spark image. We could spawn it up and run a built-in example program that&rsquo;s already built into the image (indicated by <code>local://</code>).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># First, make sure there is a role the Submission Client can use to create our resources on the cluster and that a security token corresponding to the role gets mounted to the containers. This is esp. needed for the driver container to be able to spin up executors. </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We can use the following spec which stems from the Operator repo, but is not specific to the Operator. </span>
</span></span><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/spark-on-k8s-operator/master/manifest/spark-rbac.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>export IMAGE<span style="color:#f92672">=</span>mokari94/spark-base:latest
</span></span><span style="display:flex;"><span><span style="color:#75715e"># UPDATE YOUR PORT IN THE FOLLOWING LINE (minikube cluster-info)</span>
</span></span><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --conf spark.kubernetes.container.image<span style="color:#f92672">=</span>$IMAGE <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --conf spark.kubernetes.authenticate.driver.serviceAccountName<span style="color:#f92672">=</span>spark <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --conf spark.kubernetes.container.image.pullPolicy<span style="color:#f92672">=</span>Always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  local:///opt/spark/examples/src/main/python/pi.py
</span></span></code></pre></div><p>However, as one can see, we launch a driver program that is located in the container&rsquo;s file system. While there is support to distribute an application that is local to the submission client (e. g. on the developer machine) <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#dependency-management">trough an S3-compatible object store</a>, it is not (<a href="https://issues.apache.org/jira/browse/SPARK-27936">yet?</a>) possible to <a href="(https://docs.google.com/document/d/1peg_qVhLaAl4weo5C51jQicPwLclApBsdR1To2fgc48/edit)">directly distribute the local application through Kubernetes</a>.</p>
<h2 id="custom-spark-application-image">Custom Spark Application Image</h2>
<p>However, the whole idea of Spark on Kubernetes is that we can deploy Spark applications as containers themselves.</p>
<p>So, let&rsquo;s derive a custom Application container with the following properties:</p>
<ul>
<li>Use a custom Python App with custom dependencies (namely <code>numpy</code>)</li>
<li>PySpark instead of Spark</li>
<li>Read an injected S3 path from an environment variable</li>
<li>Write the result to AWS S3 at the specified path and read it back it</li>
</ul>
<p>First, let&rsquo;s use the <code>pi.py</code> example as a starting point:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>mkdir app <span style="color:#f92672">&amp;&amp;</span> cd app
</span></span><span style="display:flex;"><span>wget https://raw.githubusercontent.com/apache/spark/master/examples/src/main/python/pi.py
</span></span><span style="display:flex;"><span>mv pi.py app.py
</span></span></code></pre></div><p>Let&rsquo;s introduce the following changes:</p>
<ul>
<li>employ <code>numpy.random.rand</code> instead of <code>random.random</code></li>
<li>load the required S3 path from an environment variable</li>
<li>dump the result to S3 and read it back in</li>
</ul>
<p>To do so, replace the script with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># app.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> operator <span style="color:#f92672">import</span> add
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession\
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>builder\
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>appName(<span style="color:#e6db74">&#34;PythonPi&#34;</span>)\
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>partitions <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span> <span style="color:#f92672">*</span> partitions
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Doing n iterations: &#34;</span>, n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(_):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> y <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>count <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>parallelize(range(<span style="color:#ae81ff">1</span>, n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>), partitions)<span style="color:#f92672">.</span>map(f)<span style="color:#f92672">.</span>reduce(add)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pi_approx <span style="color:#f92672">=</span> <span style="color:#ae81ff">4.0</span> <span style="color:#f92672">*</span> count <span style="color:#f92672">/</span> n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Pi is roughly </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> pi_approx)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Writing file to S3...&#34;</span>)
</span></span><span style="display:flex;"><span>spark \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>createDataFrame([
</span></span><span style="display:flex;"><span>        pi_approx
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;float&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>write \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv(os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;S3A_DATA_PATH&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Wrote file to S3...&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Reading file from S3...&#34;</span>)
</span></span><span style="display:flex;"><span>lines <span style="color:#f92672">=</span> spark \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>read \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>text(os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;S3A_DATA_PATH&#34;</span>)) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Read file from S3!&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>stop()
</span></span></code></pre></div><p>Let&rsquo;s manage our <code>numpy</code> dependency in an <code>env.yml</code> file using conda:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># env.yml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">python3.7-spark</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">dependencies</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">python=3.7</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">numpy</span>
</span></span></code></pre></div><p>Now, let&rsquo;s create the Docker image by deriving from our previously built Spark base image, then installing miniconda, then copying the env.yml and installing them using miniconda and finally copying the <code>app.py</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span><span style="color:#75715e"># Tag: mokari94/spark-app:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> mokari94/spark-base:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e">##### SPARK FIX #####</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Because we&#39;re not using the default start-up routine, we manually need to add a name to Spark&#39;s UID </span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># (which, yes, I hereby hardcode to the default of 185)</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># If you don&#39;t do this, you&#39;ll have hours of debugging a </span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># exception ahead of you.</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> echo <span style="color:#e6db74">&#39;185:x:185:0:anonymous uid:/opt/spark:/bin/false&#39;</span> &gt;&gt; /etc/passwd <span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e">##### CONDA #####</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Prepare installation as root</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">USER</span><span style="color:#e6db74"> 0</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> apt-get update -y <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    apt-get install -y <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        wget<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> chmod <span style="color:#ae81ff">777</span> /opt<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>    <span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Back to the default UID for Spark</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">USER</span><span style="color:#e6db74"> 185</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Install</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> wget --progress<span style="color:#f92672">=</span>dot:mega https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/miniconda3<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ENV</span> PATH<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/opt/miniconda3/bin:</span><span style="color:#e6db74">${</span>PATH<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ENV</span> PATH<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/opt/miniconda3/condabin:</span><span style="color:#e6db74">${</span>PATH<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e">##### PYTHON PACKAGE DEPENDENCIES #####</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /opt/spark/work-dir</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> env.yml env.yml<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> conda env update -f env.yml --name base<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e">##### APP #####</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ENV</span> PYTHONUNBUFFERED<span style="color:#f92672">=</span>.<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> app.py app.py<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">ENTRYPOINT</span> [ <span style="color:#e6db74">&#34;/opt/entrypoint.sh&#34;</span> ]<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>Now build and push this image to a public Docker hub (we could also use a private registry and specify a pull secret in the next step):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>export REPO<span style="color:#f92672">=</span>mokari94
</span></span><span style="display:flex;"><span>docker build -t $REPO/spark-app:latest .
</span></span><span style="display:flex;"><span>docker push $REPO/spark-app:latest
</span></span></code></pre></div><p>Alternatively, we could build the docker container <strong>on the virtualized Minikube node</strong> so that the Minikube Kubernetes will find the image using <code>eval $(minikube docker-env) &amp;&amp; docker build -t spark-app .</code>. Remember to run <code>eval $(minikube docker-env)</code> whenever you enter a fresh terminal window if you want to update the Docker image. Also be careful, to set the imagePullPolicy in the next step correctly.</p>
<h1 id="running-custom-spark-applications-and-notebooks-on-k8s">Running Custom Spark Applications and Notebooks on K8s</h1>
<h2 id="prerequisites">Prerequisites</h2>
<p>So now, all we have done is building a custom Spark application image. However, before we can run it, we need to make sure</p>
<ul>
<li>there is a Spark service account the driver can assume to create the executor pods,</li>
<li>there in an S3 bucket and prefix for the executor-created log files, so we can inspect what Spark is doing,</li>
<li>there are credentials for AWS accessible to the driver and the executors through Kubernetes Secrets, so the log files can be dumped to S3, and, in particular, so that we can read and write our data from and to S3.</li>
</ul>
<h3 id="role">Role</h3>
<p>If you haven&rsquo;t done this above, to create the service account for spark, we can reuse a spec, by the Spark operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/spark-on-k8s-operator/master/manifest/spark-rbac.yaml
</span></span></code></pre></div><p>In the spark-submit call, we will indicate the service account as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>--conf spark.kubernetes.authenticate.driver.serviceAccountName<span style="color:#f92672">=</span>spark
</span></span></code></pre></div><h3 id="log-files">Log files</h3>
<p>Let&rsquo;s create the Spark log path. We must make sure that there is at least a single file under the prefix we use (otherwise we get a FileNotFound exception in some situations, esp. when, later on, using the History Server):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>aws s3 mb s3://spark-mo/
</span></span><span style="display:flex;"><span>touch dummy
</span></span><span style="display:flex;"><span>aws s3 cp dummy s3://spark-mo/history-server/
</span></span><span style="display:flex;"><span>rm dummy
</span></span></code></pre></div><p>In the spark-submit call, we will indicate the log args as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>--conf spark.eventLog.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.eventLog.dir<span style="color:#f92672">=</span>s3a://spark-mo/history-server/
</span></span></code></pre></div><h3 id="secrets">Secrets</h3>
<p>We populate the AWS credentials from a Kubernetes secret into the container using the <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#secret-management">environment variables Spark feature</a>. In order to store the secrets as a Kubernetes secret, Put the <code>AWS_ACCESS_KEY_ID</code> into a file named <code>aws-access-key</code> and the <code>AWS_SECRET_KEY</code> into a file named <code>aws-secret-key</code>.
Make sure that there is no trailing line break.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>kubectl create secret generic aws-secrets --from-file<span style="color:#f92672">=</span>aws-access-key --from-file<span style="color:#f92672">=</span>aws-secret-key
</span></span></code></pre></div><p>In the spark-submit call, we will indicate the secrets as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>--conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span>aws-secrets:aws-access-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>aws-secrets:aws-secret-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span>aws-secrets:aws-access-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>aws-secrets:aws-secret-key
</span></span></code></pre></div><p>The above notation says to populate the <code>aws-access-key</code> value from the <code>aws-secrets</code> Kubernetes secret into an environment variable called <code>AWS_ACCESS_KEY_ID</code>. Should you rely on temporary role session tokens on the AWS side, this also could be <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-aws/tools/hadoop-aws/index.html#Using_Session_Credentials_with_TemporaryAWSCredentialsProvider">taken into account here</a>.</p>
<p>However, for the Hadoop AWS and the AWS SDK to be able to access S3, we included in our Spark Base Image, to find be able to actually make requests to the AWS API, we need to indicated the region, your bucket was created. As the region I use, Frankfurt, only offers the newer v4 Authentication Signature (v2 is legacy, anyways). it is needed to set enable Signature v4 by extra java options for driver and executor. Otherwise, we&rsquo;ll get a very non-specific <em>Bad Request</em> error. Finally, we need to set the AWS credentials.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>--conf spark.hadoop.fs.s3a.endpoint<span style="color:#f92672">=</span>s3.eu-central-1.amazonaws.com <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.executor.extraJavaOptions<span style="color:#f92672">=</span>-Dcom.amazonaws.services.s3.enableV4<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--conf spark.driver.extraJavaOptions<span style="color:#f92672">=</span>-Dcom.amazonaws.services.s3.enableV4<span style="color:#f92672">=</span>true
</span></span></code></pre></div><h2 id="submitting-applications-in-cluster-mode">Submitting Applications in Cluster Mode</h2>
<p>Finally, let&rsquo;s get to action:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>export IMAGE<span style="color:#f92672">=</span>mokari94/spark-app:latest
</span></span><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>export S3A_DATA_PATH<span style="color:#f92672">=</span>s3a://spark-mo/pi/
</span></span><span style="display:flex;"><span>spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.authenticate.driver.serviceAccountName<span style="color:#f92672">=</span>spark <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.container.image<span style="color:#f92672">=</span>$IMAGE <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.container.image.pullPolicy<span style="color:#f92672">=</span>Always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.executor.instances<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.executor.cores<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.task.maxFailures<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.executor.memory<span style="color:#f92672">=</span>1g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.driver.memory<span style="color:#f92672">=</span>1g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span>aws-secrets:aws-access-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>aws-secrets:aws-secret-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span>aws-secrets:aws-access-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>aws-secrets:aws-secret-key <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.hadoop.fs.s3a.endpoint<span style="color:#f92672">=</span>s3.eu-central-1.amazonaws.com <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.executor.extraJavaOptions<span style="color:#f92672">=</span>-Dcom.amazonaws.services.s3.enableV4<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.driver.extraJavaOptions<span style="color:#f92672">=</span>-Dcom.amazonaws.services.s3.enableV4<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.eventLog.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.eventLog.dir<span style="color:#f92672">=</span>s3a://spark-mo/history-server/ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.kubernetes.driverEnv.S3A_DATA_PATH<span style="color:#f92672">=</span>$S3A_DATA_PATH <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --conf spark.executorEnv.S3A_DATA_PATH<span style="color:#f92672">=</span>$S3A_DATA_PATH <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    local:///opt/spark/work-dir/app.py
</span></span></code></pre></div><p>After submitting you application, you can easily observe driver and executor creation through the Kubernetes dashboard (<code>minikube dashboard</code>) and follow the log output. The driver&rsquo;s log looks this:</p>
<p><img src="fig-spark-operator-result-log.png" alt=""></p>
<p>In S3 under <code>s3a://spark-mo/pi/</code>, there should be a new or updated set of result CSVs and an empty _SUCCESS flag file. Also, under <code>s3a://spark-mo/history-server/</code> there a should be new log file.</p>
<p>Because this spark-submit call is a bit bulky, we shorten it a bit using a properties file, e. g. named spark-app.conf:</p>
<pre tabindex="0"><code class="language-conf" data-lang="conf">spark.kubernetes.container.image                                mokari94/spark-app:latest
spark.eventLog.dir                                              s3a://spark-mo/history-server/
spark.hadoop.fs.s3a.endpoint                                    s3.eu-central-1.amazonaws.com
spark.kubernetes.driverEnv.S3A_DATA_PATH                        s3a://spark-mo/pi/
spark.executorEnv.S3A_DATA_PATH                                 s3a://spark-mo/pi/
spark.kubernetes.authenticate.driver.serviceAccountName         spark
spark.kubernetes.container.image.pullPolicy                     Always
spark.executor.instances                                        2
spark.executor.cores                                            1
spark.task.maxFailures                                          3
spark.executor.memory                                           1g
spark.driver.memory                                             1g
spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID          aws-secrets:aws-access-key
spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY      aws-secrets:aws-secret-key
spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID        aws-secrets:aws-access-key
spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY    aws-secrets:aws-secret-key
spark.executor.extraJavaOptions                                 -Dcom.amazonaws.services.s3.enableV4=true
spark.driver.extraJavaOptions                                   -Dcom.amazonaws.services.s3.enableV4=true
spark.eventLog.enabled                                          true
</code></pre><p>and adapt spark-submit as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --properties-file spark-app.conf <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    local:///opt/spark/work-dir/app.py
</span></span></code></pre></div><p>Doing the same thing through the operator is straight-forward. Helm-install it, manually translate your the spark-app.conf into a Kubernetes-Manifest spark-app.yaml and run <code>kubectl apply -f spark-app.yaml</code> which will, under the hood, do the same thing, we just did above.</p>
<h2 id="running-notebooks-in-client-mode">Running Notebooks in Client Mode</h2>
<h3 id="overview-1">Overview</h3>
<p>Remember that it is important to have the same Spark distributions on your Notebook server (in this case, your developer as on the executors) and your local machine to run a notebook.
We ensured this by building our local distribution into the Spark Base Image.</p>
<p>The more production-ready alternatives would be either</p>
<ol>
<li>to clone the Spark repo,</li>
<li>build a Spark distribution from source,</li>
<li>build a Spark Base Docker image using this Spark distribution, and</li>
<li>running this specific distribution on the developer machine.</li>
</ol>
<p>However, for the moment, let&rsquo;s go with the original setup where typing <code>realpath $(which spark-submit)</code> on the will show the path to the same Spark distribution that was built into the Spark Base image. While Spark itself is written in Scala and compiled to bytecode that is always executed in a JVM, PySpark is a Python application that exposes a Python API wrapping the Java API used to communicate with the JVM. It is essential that not only the driver&rsquo;s and executor&rsquo;s Spark environment in terms of version and dependencies match, but also the Python environment.</p>
<p>So, let&rsquo;s add the jupyter and notebook dependencies to the <code>env.yml</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># We use the env.yml for the developer machine&#39;s environment, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and in the Spark App image.</span>
</span></span><span style="display:flex;"><span>cat <span style="color:#e6db74">&lt;&lt;EOF &gt; env.yml
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">name: python3.7-spark
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">dependencies:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  - python=3.7
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  - jupyter
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  - notebook
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  - numpy
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">EOF</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>conda env update -f env.yml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>python -m ipykernel install --user --name python3.7-spark
</span></span></code></pre></div><h3 id="repl-shell">REPL Shell</h3>
<p>When running the driver on the cluster, Kubernetes will make sure that the secrets defined in Spark submit are populated to the driver.
Now that we&rsquo;re controlling the driver ourselves, we need to populate the secrets ourselves.
To make sure, the executors run python3, set the env var <code>PYSPARK_PYTHON=python3</code>.
Now, let&rsquo;s see if our local developer machine can play the same role as a driver on the cluster:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>source activate python3.7-spark
</span></span><span style="display:flex;"><span>export AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-access-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-secret-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>PYSPARK_PYTHON<span style="color:#f92672">=</span>python3 spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --properties-file spark-app.conf <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --deploy-mode client <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    app.py
</span></span></code></pre></div><p>This should behave like it did on the driver pod previously.</p>
<p>However, we actually don&rsquo;t want to submit a ready application, but instead run an interactive driver program.
There, instead of  spark-submit, we use <code>pyspark</code> (the equivalent in Scala is more meaningfully called <em>spark-shell</em>).
This will always run in client mode, however - by default - run a REPL shell.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>source activate python3.7-spark
</span></span><span style="display:flex;"><span>export AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-access-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-secret-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>PYSPARK_PYTHON<span style="color:#f92672">=</span>python3 pyspark <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --properties-file spark-app.conf
</span></span></code></pre></div><p>It should look something like this:</p>
<p><img src="fig-spark-on-k8s-repl.png" alt=""></p>
<h3 id="jupyter-notebook">Jupyter Notebook</h3>
<p>Finally, let&rsquo;s run this in Jupyter by instructing PySpark through environment variables to use run <code>jupyter</code> and pass the <code>notebook .</code> args:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>source activate python3.7-spark
</span></span><span style="display:flex;"><span>export AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-access-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat ../aws-secret-key<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>export K8S_API_URL<span style="color:#f92672">=</span>https://127.0.0.1:32776
</span></span><span style="display:flex;"><span>PYSPARK_PYTHON<span style="color:#f92672">=</span>python3 PYSPARK_DRIVER_PYTHON<span style="color:#f92672">=</span>jupyter PYSPARK_DRIVER_PYTHON_OPTS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;notebook .&#34;</span> pyspark <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --master k8s://$K8S_API_URL <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --properties-file spark-app.conf
</span></span></code></pre></div><p>Create a new notebook with as <code>python3.7-spark</code> as an environment.</p>
<p>Copy the following into a cell and run it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>setLogLevel(<span style="color:#e6db74">&#34;INFO&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> random <span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> operator <span style="color:#f92672">import</span> add
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>partitions <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span> <span style="color:#f92672">*</span> partitions
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Doing n iterations: &#34;</span>, n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(_):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> random() <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> random() <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> y <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>count <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>parallelize(range(<span style="color:#ae81ff">1</span>, n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>), partitions)<span style="color:#f92672">.</span>map(f)<span style="color:#f92672">.</span>reduce(add)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pi_approx <span style="color:#f92672">=</span> <span style="color:#ae81ff">4.0</span> <span style="color:#f92672">*</span> count <span style="color:#f92672">/</span> n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Pi is roughly </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> pi_approx)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Writing file to S3...&#34;</span>)
</span></span><span style="display:flex;"><span>spark \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>createDataFrame([
</span></span><span style="display:flex;"><span>        pi_approx
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;float&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>write \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;s3a://spark-mo/pi/&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Wrote file to S3...&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Reading file from S3...&#34;</span>)
</span></span><span style="display:flex;"><span>lines <span style="color:#f92672">=</span> spark \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>read \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>text(<span style="color:#e6db74">&#34;s3a://spark-mo/pi/&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Read file from S3!&#34;</span>)
</span></span></code></pre></div><p>As seen below, <em>print</em> statements are displayed in the notebook but the driver log is given in the terminal window.</p>
<p><img src="fig-spark-on-k8s-jupyter.png" alt=""></p>
<p>Notice, that the code from within the Jupyter notebook is now actually marshaled and sent over to the executors on the cluster.
Of course, if you now depend on a package in your Python code, it has to be part of the Spark App container so each executor can use it.</p>
<p>Also note, that now, since - the driver is running on the developer machine, without further ado, you can check the driver&rsquo;s Spark UI, by default running on http://localhost:4040/.</p>
<p>Alternative to the <code>pyspark</code> command, after starting a blank notebook, one could use</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark <span style="color:#f92672">import</span> SparkConf, SparkContext
</span></span><span style="display:flex;"><span>conf <span style="color:#f92672">=</span> SparkConf()
</span></span><span style="display:flex;"><span>conf<span style="color:#f92672">.</span>setMaster(<span style="color:#e6db74">&#34;k8s://...&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># conf.set(&#34;...&#34;, &#34;...&#34;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SparkContext(conf<span style="color:#f92672">=</span>conf)
</span></span></code></pre></div><p>However, we will build the Jupyter notebook server into a Docker image and therefore it makes sense to already exercise calling it from the command line. Why do we need to run it as a container anyway?</p>
<p>Now, that we have see how all of this works, there&rsquo;s unfortunately a show-stopper to the setup.
The driver needs to be network-addressable from the executors.
While this is the case when running on Minikube, the Jupyter notebook server, in general, won&rsquo;t be network-addressable when running on a real cluster at AWS, at least if we&rsquo;re running it on the developer machine. Of course, we could run the notebook server on a gateway server in the same AWS VPC, however why not run the notebook server on Kubernetes itself?</p>
<p>While we could build a separate Docker image for the notebooks, let&rsquo;s make our life easier and build notebook support into our Spark App image. This singlehandedly ensures that the driver running in the Jupyter Notebook container and the executors run the exact same Spark and Python dependencies. Ahh, containers are just beautiful.</p>
<p>So, what do we need in order to run Jupyter in a Docker container?
Spark? Check. Python? Check (through conda)! The Python <code>jupyter</code> and <code>notebook</code> PyPI packages? Check!!!</p>
<p>The following Kubernetes manifest will spin up a single Pod running the notebook server (PRs on Github welcome :) ).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># spark-notebook.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">spark-notebook</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">role</span>: <span style="color:#ae81ff">spark-notebook</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceAccountName</span>: <span style="color:#ae81ff">spark</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">spark-notebook</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">image</span>: <span style="color:#ae81ff">mokari94/spark-app:latest</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">HOME</span> <span style="color:#75715e"># overwrites HOME so Jupyter will for sure have permission</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;/tmp/.jupyter&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">PYSPARK_DRIVER_PYTHON</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;jupyter&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">PYSPARK_DRIVER_PYTHON_OPTS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;notebook . --ip 0.0.0.0&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">AWS_ACCESS_KEY_ID</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">valueFrom</span>: 
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">name</span>: <span style="color:#ae81ff">aws-secrets</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">key</span>: <span style="color:#ae81ff">aws-access-key</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">AWS_SECRET_ACCESS_KEY</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">valueFrom</span>: 
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">secretKeyRef</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">name</span>: <span style="color:#ae81ff">aws-secrets</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">key</span>: <span style="color:#ae81ff">aws-secret-key</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">command</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">/opt/spark/bin/pyspark ,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">master, k8s://https://kubernetes:443,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">conf, spark.jars.ivy=/tmp/.ivy,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">conf, spark.kubernetes.container.image=mokari94/spark-app:latest,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">conf, spark.driver.port=40694,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">conf, spark.driver.host=spark-notebook-service,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">conf, spark.kubernetes.driver.pod.name=spark-notebook,</span>
</span></span><span style="display:flex;"><span>        --<span style="color:#ae81ff">properties-file, /etc/config/spark-notebook.conf</span>
</span></span><span style="display:flex;"><span>      ]
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># spark.driver.host sets the name of the headless service</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># executors will need to send their communication to the driver process</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># which we want to be on the Jupyter server.</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># However, pod names are not directly resolvable to the host.</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># So instead, we use a service that then routes to the pod.</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># Instead of using a &#34;standard&#34; service that can loadbalance across multiple pods, </span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># we use a headless service that simply passes through requests to the pod that matches the selector.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8888</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">40694</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">config-volume</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/etc/config/</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">config-volume</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">configMap</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">spark-notebook-configmap</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">spark-notebook-service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">clusterIP</span>: <span style="color:#ae81ff">None</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">role</span>: <span style="color:#ae81ff">spark-notebook</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8888</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8888</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">jupyter-web-ui</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">40694</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">40694</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">spark-driver-port</span>
</span></span></code></pre></div><p>Run the notebook using</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>kubectl apply -f spark-notebook.yaml
</span></span></code></pre></div><p>Use the <code>minikube service</code> or <code>kubectl port-forward service/spark-notebook-service 8888:8888</code> command to access the notebook through your browser under <code>localhost:8888</code>.</p>
<p>The spark driver running inside the notebook container will be able to spin up executors because the manifest assigns the spark service role to the pod. This mounts the token for the API server at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. In order to access the notebook, copy the token itself from the pod&rsquo;s log. Also remember that the pod&rsquo;s local file system will be deleted when the pod is deleted, so using git could be a good idea.</p>
<h1 id="spark-driver-ui-and-the-history-server">Spark Driver UI and the History Server</h1>
<p>By using port-forwarding, we could also check the driver&rsquo;s Spark UI for jobs submitted to Spark on K8s in cluster-mode or for the cluster-deployed notebook server.
However the central problem is that the Spark UI is coupled to the Spark driver process, meaning that once the process ends, the UI is no longer served.
While we can still access the raw log files themselves in the AWS S3 bucket we configure in the <code>spark-submit</code> or <code>pyspark</code> call, accessing them visually through the UI is a bit more handy, isn&rsquo;t it?</p>
<p>Here, the history server comes to rescue.
It is deployed as a separate component, not necessarily but possibly on the same K8s cluster. It&rsquo; basically just a visual log file explorer, that parses and displays the logs stored - in our case - in S3 by the driver and executors. Therefore, it is necessary that the history server as well as the driver and executors have access to the same AWS S3 bucket, so that the latter can write to it and that the former can read from it.</p>
<p>In theory, we can use the Spark Base image to run the History server, because we built in the dependencies (hadoop-aws and aws-java-sdk-bundle) for AWS access.
However, when actually doing so it throws a <code>javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name</code> due to the fact that the default Spark user in the linux container is unnamed. In the App image, we have added the line</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>echo <span style="color:#e6db74">&#39;185:x:185:0:anonymous uid:/opt/spark:/bin/false&#39;</span> &gt;&gt; /etc/passwd 
</span></span></code></pre></div><p>to circumvent this. So let&rsquo;s simply reuse the app image.</p>
<p>To deploy the history server, let&rsquo;s use Helm.</p>
<p>First, make the official Helm chart repo available.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>helm repo add stable https://kubernetes-charts.storage.googleapis.com
</span></span></code></pre></div><p>Let&rsquo;s have a look at a config template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>helm show values stable/spark-history-server &gt; history-server-config.yaml
</span></span></code></pre></div><p>From this, let&rsquo;s use the following subset:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">image</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">mokari94/spark-app</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tag</span>: <span style="color:#ae81ff">latest</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">Always</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">environment</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">SPARK_HISTORY_OPTS</span>:
</span></span><span style="display:flex;"><span>    -<span style="color:#ae81ff">Dcom.amazonaws.services.s3.enableV4=true</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">nfs</span>: 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enableExampleNFS</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">pvc</span>: 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enablePVC</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">s3</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enableS3</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enableIAM</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">aws-secrets</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessKeyName</span>: <span style="color:#ae81ff">aws-access-key</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">secretKeyName</span>: <span style="color:#ae81ff">aws-secret-key</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">logDirectory</span>: <span style="color:#ae81ff">s3a://spark-mo/history-server/</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">endpoint</span>: <span style="color:#ae81ff">s3.eu-central-1.amazonaws.com</span>
</span></span></code></pre></div><p>Now, let&rsquo;s deploy the history server using:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>helm install -f history-server-config.yaml spark-history-server stable/spark-history-server
</span></span></code></pre></div><p>and finally, expose it to the outside using</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>minikube service spark-history-server
</span></span></code></pre></div><p><img src="fig-spark-on-k8s-history.png" alt=""></p>
<h1 id="outlook">Outlook</h1>
<p>In this post, we&rsquo;ve examined how to setup a fully containerized, PySpark-based, History-Server-monitored workflow for submitting Spark applications and interactively running Spark-backed notebooks on Kubernetes. While understanding and elaborating on the factors at play made this a quite lengthy post, I think the <a href="https://github.com/MohamedKari/spark-on-k8s">resulting GitHub repo</a>. I always strive to avoid trivial examples that do not capture the complexities of going into the real world of bordering systems, large data sets, networks, code dependencies, etc. So, instead of using toy examples that are incommensurable with real problems, I still present toy examples that do not distract from the big picture however while <em>respecting the challenging facets</em> of the real world. There are of course still quite some relaxations in the previous remarks incl.</p>
<ul>
<li>not building from source and therefore relying on a highly fragile manual dependency management (it would be interesting to have a look at Docker Multistage builds for this).</li>
<li>assuming a dedicated Kubernetes cluster, thus ignoring roles and the concept of Namespaces and installing everything to default,</li>
<li>not regarding security (e. g. by using TLS)</li>
<li>not taking a look at integrations, e. g. with <a href="https://github.com/databricks/koalas">Databricks&rsquo; Koalas library</a> for a pandas-like user experience on Spark.</li>
</ul>
<p>In the next post, we&rsquo;ll see how to take everything learned so far together, to transform 1 Terabyte of the Waymo Open Dataset, consisting of about 1000 driving sequences in protobuf format, to the RGB camera frames and labels only, using 150 executors running on an EKS cluster for consumption.</p>
<p><img src="fig-spark-on-k8s-waymo.png" alt=""></p>

  </section>
  <footer>
    
    <nav class="p-pagination c-pagination">
      <div class="c-pagination__ctrl">
        <div class="c-pagination__newer">
          
          <a href="https://mohamedkari.github.io/blog.mkari.de/posts/glx-on-mac/">Newer</a>
          
        </div>
        <div class="c-pagination__older">
          
          <a href="https://mohamedkari.github.io/blog.mkari.de/posts/reproducible-ml-models-using-docker/">Older</a>
          
        </div>
      </div>
    </nav>
    


  </footer>
</article>
  </main>
  

  <footer class="l-footer">
    
<ul class="c-links">
  
  
  
  
  
  <li class="c-links__item">
    <a href="https://github.com/MohamedKari" target="_blank">
      <svg viewBox="0 0 64 64" class="c-links__icon">
        <title>github</title>
        <use xlink:href="#icon-github"></use>
      </svg>
    </a>
  </li>
  
  
  
  
  
  
  
  
  
  
  
  
  <li class="c-links__item">
    <a href="https://linkedin.com/in/mohamedkari" target="_blank">
      <svg viewBox="0 0 64 64" class="c-links__icon">
        <title>linkedin</title>
        <use xlink:href="#icon-linkedin"></use>
      </svg>
    </a>
  </li>
  
</ul>



    <p class="p-copyright">
      
        &copy; Mohamed Kari&nbsp;‚Ä¢&nbsp;All rights reserved.&nbsp;‚Ä¢&nbsp;2020
      
    </p>
  </footer>


  <script defer type="module">
    
    let pairs = document.cookie.split(";")
    let ref_list = pairs.map(pair => pair.split("=")).filter(splitted => splitted[0].trim() == "ref")

    let ref = ""
    if (ref_list.length == 0) {
      ref = String(Math.random()).replace(".", "")
      var new_cookie = "ref=" + ref + ";path=/;expires=Wed, 01 Dec 2022 01:15:29 GMT;"
      
      document.cookie = new_cookie 
    }¬†else {
      ref = ref_list[0][1]
      
    }
    

    let url="https://api.mkari.de/counter/" + document.location.pathname.replaceAll("/", "_") + "?ref=" + ref
    fetch(url, {
      method: "POST"
    })

  </script>

</body>
</html>

