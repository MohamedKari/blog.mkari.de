<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://mohamedkari.github.io/blog.mkari.de/posts/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Mohamed Kari&amp;nbsp;•&amp;nbsp;All rights reserved.&amp;nbsp;•&amp;nbsp;2020</copyright>
    <lastBuildDate>Wed, 18 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mohamedkari.github.io/blog.mkari.de/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Securing a containerized Flask API with Let&#39;s Encrypt Certificates</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/secure-apis/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/secure-apis/</guid>
      <description>Using Certbot, Nginx, and Flask, each running in a Docker container spun up through Docker Compose, this post shows how to serve an API over HTTPS conveniently with Let&amp;rsquo;s Encrypt certificates. Template repo available under &lt;a href=&#34;https://github.com/MohamedKari/secure-flask-container-template&#34;&gt;https://github.com/MohamedKari/secure-flask-container-template&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Running an X Server with Indirect GLX Rendering on MacOS for containerized applications with GUIs</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/glx-on-mac/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/glx-on-mac/</guid>
      <description>Intro For my latest research, I am looking into visual SLAM (e. g. ORB-SLAM2). Since VSLAM libraries are designated for running efficiently on embedded systems, it is generally programmed in C/C++ and designed with just Linux in mind (even though, for example, in version 2, ROS also aims for compatibility with MacOS).
As a MacBook user, this becomes &amp;ldquo;interesting&amp;rdquo;. Of course, Docker makes it easy to run libraries for Linux. However, once the software also comprises graphical UIs, e.</description>
    </item>
    
    <item>
      <title>Native Spark on Kubernetes</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/spark-on-k8s/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/spark-on-k8s/</guid>
      <description>In this post, the different deployment alternatives of Spark on Kubernetes are evaluated. From this, I&amp;rsquo;ll outline the workflow for building and running Spark Applications as well as Spark Cluster-backed Jupyter Notebooks, both running PySpark in custom containers. It is shown how to include conda-managed Python dependencies in the image. Also, it is described how to deploy a notebook server running in Spark&amp;rsquo;s client mode to the Kubernetes cluster. Workloads use AWS S3 as the data source and sink and are observable using the Spark history server.</description>
    </item>
    
    <item>
      <title>Reproducible ML Models using Docker</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/reproducible-ml-models-using-docker/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/reproducible-ml-models-using-docker/</guid>
      <description>Reproducing ML models can be a pain. And this is not even talking about managing model reproducibility with different datasets, features, hyperparameters, architectures, setups, non-deterministic optimization or about model reproducibility in a production-ready setup with constantly evolving input data. No, what I am talking about is getting a model which was developed and published by a different researcher to run on your own machine. Sometimes, or more like most times, this can be a nerve-wrecking endeavor.</description>
    </item>
    
    <item>
      <title>Remote Docker Hosts in the Cloud for Machine Learning Workflows</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/remote-docker-for-ml/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/remote-docker-for-ml/</guid>
      <description>The problem of developing ML models on a MacBook In a recent blog post, I have argued why I think it is a good idea to develop ML models inside Docker containers. In short: reproducibility. However, if you don&amp;rsquo;t have access to a CUDA-enabled GPU, developing or even only replicating state-of-the-art deep-learning research can be close to impossible, Docker or not. All ML researchers and engineers working on a MacBook have probably been exposed to this complication.</description>
    </item>
    
    <item>
      <title>Out-of-the-box Storage Infrastructure Alternatives for Scaled Machine Learning</title>
      <link>https://mohamedkari.github.io/blog.mkari.de/posts/storage/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mohamedkari.github.io/blog.mkari.de/posts/storage/</guid>
      <description>The problem of storing large volumes of unstructured datasets Data preprocessing is a vital part of machine learning workflows. However, the story starts even earlier. Even before versioning or labelling data, we have to store the data we want learn from. This quickly becomes a non-trivial task in deep-learning problems where we often operate on non-tabular data such as images resulting in terabyte-scale dataset sizes such as the Waymo dataset for example.</description>
    </item>
    
  </channel>
</rss>